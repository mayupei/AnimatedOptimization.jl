<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="schrimpf">
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Notes on Algorithms - AnimatedOptimization.jl</title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atelier-forest-light.min.css">
        <link href="../assets/Documenter.css" rel="stylesheet">

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="..">AnimatedOptimization.jl</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href=".." class="nav-link">Package Documentation</a>
                            </li>
                            <li class="navitem active">
                                <a href="./" class="nav-link">Notes on Algorithms</a>
                            </li>
                            <li class="navitem">
                                <a href="../exercises/" class="nav-link">Exercises</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                            <li class="nav-item">
                                <a rel="prev" href=".." class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../exercises/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/schrimpf/AnimatedOptimization.jl/edit/master/docs/optimization.md" class="nav-link"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            
            <li class="nav-item" data-level="1"><a href="#optimization-algorithms" class="nav-link">Optimization Algorithms</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#heuristic-searches" class="nav-link">Heuristic searches</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#gradient-descent" class="nav-link">Gradient descent</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#newtons-method" class="nav-link">Newton's method</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#line-search" class="nav-link">Line search</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#trust-region" class="nav-link">Trust region</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#quasi-newton" class="nav-link">Quasi-Newton</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#details-matter-in-practice" class="nav-link">Details matter in practice</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#constrained-optimization" class="nav-link">Constrained optimization</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#interior-point-methods" class="nav-link">Interior Point Methods</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#sequential-quadratic-programming" class="nav-link">Sequential quadratic programming</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#slqp-active-set" class="nav-link">SLQP active Set</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#augmented-lagrangian" class="nav-link">Augmented Lagrangian</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#barrier-methods" class="nav-link">Barrier methods</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#strategies-for-global-optimization" class="nav-link">Strategies for global optimization</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#references" class="nav-link">References</a>
              <ul class="nav flex-column">
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a></p>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike
4.0 International
License</a> </p>
<h3 -="-" id="about-this-document">About this document<a class="headerlink" href="#about-this-document" title="Permanent link">&para;</a></h3>
<p>This document was created using Weave.jl. The code is available 
<a href="https://github.com/schrimpf/AnimatedOptimization.jl/">on github</a>. The same
document generates both the <a href="https://schrimpf.github.io/AnimatedOptimization.jl/optimization/">static webpage</a> 
and associated <a href="https://schrimpf.github.io/AnimatedOptimization.jl/optimization.ipynb">jupyter notebook</a> (<a href="https://nbviewer.jupyter.org/urls/schrimpf.github.io/AnimatedOptimization.jl/optimization.ipynb">or on nbviewer</a>).</p>
<p>
<script type="math/tex; mode=display">
\def\indep{\perp\!\!\!\perp}
\def\Er{\mathrm{E}}
\def\R{\mathbb{R}}
\def\En{{\mathbb{E}_n}}
\def\Pr{\mathrm{P}}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\inprob{\,{\buildrel p \over \rightarrow}\,} 
\def\indist{\,{\buildrel d \over \rightarrow}\,} 
</script>
</p>
<h1 id="optimization-algorithms">Optimization Algorithms<a class="headerlink" href="#optimization-algorithms" title="Permanent link">&para;</a></h1>
<p>The goal of this notebook is to give you some familiarity with numeric
optimization. </p>
<p>Numeric optimization is important because many (most) models cannot be
fully solved analytically. Numeric results can be used to complement
analytic ones. Numeric optimization plays a huge role in econometrics. </p>
<p>In these notes, we will focus on minimization problems following the
convention in mathematics, engineering, and most numerical
libraries. It is easy to convert between minimization and
maximization, and we hope that this does not lead to any confusion.</p>
<h1 id="heuristic-searches">Heuristic searches<a class="headerlink" href="#heuristic-searches" title="Permanent link">&para;</a></h1>
<p>The simplest type of optimization algorithm are heuristic
searches. Consider the problem: </p>
<p>
<script type="math/tex; mode=display">
\min_x f(x)
</script>
</p>
<p>with $f:\R^n \to \R$. Heuristic search algorithms consist of </p>
<ol>
<li>Evaluate $f$ at a collection of points </li>
<li>Generate a new candidate point, $x^{new}$. Replace a point
   in the current collection with $x^{new}$ if $f(x^{new})$ is small enough. </li>
<li>Stop when function value stops decreasing and/or collection of
   points become too close together. </li>
</ol>
<p>There are many variants of such algorithms with different ways of
generating new points, deciding whether to accept the new point, and
deciding when to stop.  Here is a simple implementation and animation
of the above idea. In the code below, new points are drawn randomly
from a normal distribution, and new points are accepted whenever
$f(x^{new})$ is smaller than the worst existing function value. </p>
<pre><code class="language-julia">markdown &amp;&amp; printfunc(&quot;minrandomsearch&quot;,&quot;heuristic_optimizers.jl&quot;)
</code></pre>
<pre><code>&quot;&quot;&quot;
    minrandomsearch(f, x0, npoints; var0=1.0, ftol = 1e-6,
                         vtol = 1e-4, maxiter = 1000,
                         vshrink=0.9, xrange=[-2., 3.],
                         yrange=[-2.,6.])

Find the minimum of function `f` by random search.

Creates an animation illustrating search progress.

# Arguments

- `f` function to minimize
- `x0` starting value
- `npoints` number of points in cloud
- `var0` initial variance of points
- `ftol` convergence tolerance for function value. Search terminates if bot
h function change is less than ftol and variance is less than vtol.
- `vtol` convergence tolerance for variance. Search terminates if both func
tion change is less than ftol and variance is less than vtol.
- `maxiter` maximum number of iterations
- `vshrink` after every 100 iterations with no function improvement, the va
riance is reduced by this factor
- `xrange` range of x-axis in animation
- `yrange` range of y-axis in animation
- `animate` whether to create animation

# Returns

- `(fmin, xmin, iter, info, anim)` tuple consisting of minimal function
  value, minimizer, number of iterations, convergence info, and an animatio
n
&quot;&quot;&quot;
function minrandomsearch(f, x0, npoints; var0=1.0, ftol = 1e-6,
                         vtol = 1e-4, maxiter = 1000,
                         vshrink=0.9, xrange=[-2., 3.],
                         yrange=[-2.,6.], animate=true)
  var = var0     # current variance for search
  oldbest = Inf  # smallest function value
  xbest = x0     # x with smallest function vale
  newbest = f(xbest)
  iter = 0       # number of iterations
  noimprove = 0  # number of iterations with no improvement

  animate = (animate &amp;&amp; length(x0)==2)

  if animate
    # make a contour plot of the function we're minimizing. This is for
    # illustrating; you wouldn't have this normally
    x = range(xrange[1],xrange[2], length=100)
    y = range(yrange[1],yrange[2], length=100)
    c = contour(x,y,(x,y) -&gt; log(f([x,y])))
    anim = Animation()
  else
    anim = nothing
  end

  while ((oldbest - newbest &gt; ftol || var &gt; vtol) &amp;&amp; iter&lt;=maxiter)
    oldbest = newbest
    x = rand(MvNormal(xbest, var),npoints)

    if animate
      # plot the search points
      p = deepcopy(c)
      scatter!(p, x[1,:], x[2,:], markercolor=:black, markeralpha=0.5, lege
nd=false, xlims=xrange, ylims=yrange)
    end

    fval = mapslices(f,x, dims=[1])
    (newbest, i) = findmin(fval)
    if (newbest &gt; oldbest)
      noimprove+=1
      newbest=oldbest
    else
      xbest = x[:,i[2]]
      noimprove = 0
    end

    if animate
      # plot the best point so far
      scatter!(p, [xbest[1]],[xbest[2]], markercolor=:red, legend=false)
    end

    if (noimprove &gt; 10) # shrink var
      var *= vshrink
    end

    if animate
      frame(anim) # add frame to animation
    end

    iter += 1
  end
  if (iter&gt;maxiter)
    info = &quot;Maximum iterations reached&quot;
  else
    info = &quot;Convergence.&quot;
  end
  return(minimum = newbest, minimizer = xbest, iters = iter, info = info, a
nim = anim)
end
</code></pre>
<pre><code class="language-julia">&quot;&quot;&quot;
     banana(a,b)

  Returns the Rosenbrock function with parameters a, b.
&quot;&quot;&quot;
function banana(a,b)
  x-&gt;(a-x[1])^2+b*(x[2]-x[1]^2)^2
end
f = banana(1.0,1.0)

x0 = [-2.0, 3.0];
</code></pre>
<pre><code class="language-julia">result = minrandomsearch(f, x0, 20, var0=0.1, vshrink=0.5, vtol=1e-3 )
gif(result[5], &quot;randsearch.gif&quot;, fps=5);
</code></pre>
<p><img alt="random search" src="../randsearch.gif" /></p>
<p>There are many other heuristic search algorithms. A popular
deterministic one is the Nelder-Mead simplex. Popular heuristic
search algorithms that include some randomness include simulated
annealing and particle swarm. Each of the three algorithms just
mentioned are available in
<a href="https://julianlsolvers.github.io/Optim.jl/stable/#algo/nelder_mead/">Optim.jl</a>. These
heuristic searches have the advantage that they only function values
(as opposed to also requiring gradients or hessians, see
below). Some heuristic algorithms, like simulated annealing, can be
shown to converge to a global (instead of local) minimum under
appropriate assumptions. Compared to algorithms that use more
information, heuristic algorithms tend to require many more function
evaluations. </p>
<h1 id="gradient-descent">Gradient descent<a class="headerlink" href="#gradient-descent" title="Permanent link">&para;</a></h1>
<p>Gradient descent is an iterative algorithm to find a local minimum. As
the name suggests, it consists of descending toward a minimum in the
direction opposite the gradient. Each step, you start at some $x$ and
compute $x_{new}$</p>
<ol>
<li>Given current $x$, compute $x_{new} = x - \gamma Df_{x}$</li>
<li>Adjust $\gamma$ depending on whether $f(x_{new})&lt;f(x)$</li>
<li>Repeat until $\norm{Df_{x}}$, $\norm{x-x_{new}}$, and/or
   $\abs{f(x)-f(x_{new})}$ small.</li>
</ol>
<pre><code class="language-julia">using ForwardDiff, LinearAlgebra
</code></pre>
<pre><code class="language-julia">markdown &amp;&amp; printfunc(&quot;graddescent&quot;,&quot;smooth_optimizers.jl&quot;)
</code></pre>
<pre><code>&quot;&quot;&quot;
     graddescent(f, x0; grad=x-&gt;Forwardiff.gradient(f,x),
                 γ0=1.0, ftol = 1e-6,
                 xtol = 1e-4, gtol=1-6, maxiter = 1000, 
                 xrange=[-2., 3.],
                 yrange=[-2.,6.], animate=true)

Find the minimum of function `f` by gradient descent

# Arguments

- `f` function to minimize
- `x0` starting value
- `grad` function that computes gradient of `f`
- `γ0` initial step size multiplier
- `ftol` tolerance for function value
- `xtol` tolerance for x
- `gtol` tolerance for gradient. Convergence requires meeting all three tol
erances.
- `maxiter` maximum iterations
- `xrange` x-axis range for animation
- `yrange` y-axis range for animation
- `animate` whether to create animation

# Returns

- `(fmin, xmin, iter, info, anim)` tuple consisting of minimal function
  value, minimizer, number of iterations, convergence info, and animations
&quot;&quot;&quot;
function graddescent(f, x0; grad=x-&gt;ForwardDiff.gradient(f,x),
                     γ0=1.0, ftol = 1e-6,
                     xtol = 1e-4, gtol=1-6, maxiter = 1000, 
                     xrange=[-2., 3.],
                     yrange=[-2.,6.], animate=true)
  fold = f(x0)
  xold = x0
  xchange=Inf
  fchange=Inf
  γ = γ0
  iter = 0
  stuck=0
  improve = 0 # we increase γ if 5 steps in a row improve f(x)

  animate = animate &amp;&amp; (length(x0)==2)

  if animate
    # make a contour plot of the function we're minimizing. This is for
    # illustrating; you wouldn't have this normally
    c = contour(range(xrange[1],xrange[2], length=100),
                range(yrange[1],yrange[2], length=100),
                (x,y) -&gt; log(f([x,y])))
    anim = Animation()
  else
    anim = nothing
  end
  g = grad(xold)

  while(iter &lt; maxiter &amp;&amp; ((xchange&gt;xtol) || (fchange&gt;ftol) || (stuck&gt;0)
                           || norm(g)&gt;gtol) )
    g = grad(xold)
    x = xold - γ*g
    fnew = f(x)

    if animate
      scatter!(c, [xold[1]],[xold[2]], markercolor=:red, legend=false,
               xlims=xrange, ylims=yrange) 
      quiver!(c, [xold[1]],[xold[2]], quiver=([-γ*g[1]],[-γ*g[2]]), legend=
false,
              xlims=xrange, ylims=yrange)
      frame(anim)
    end

    if (fnew&gt;=fold)
      γ*=0.5
      improve = 0     
      stuck += 1
      if (stuck&gt;=10)
        break
      end
    else
      stuck = 0
      improve += 1
      if (improve&gt;5)
        γ *= 2.0
        improve=0
      end
      xold = x
      fold = fnew
    end
    xchange = norm(x-xold)
    fchange = abs(fnew-fold)
    iter += 1
  end
  if (iter &gt;= maxiter)
    info = &quot;Maximum iterations reached&quot;
  elseif (stuck&gt;0)
    info = &quot;Failed to improve for &quot; * string(stuck) * &quot; iterations.&quot;
  else
    info = &quot;Convergence.&quot;
  end
  return(fold, xold, iter, info, anim) 
end
</code></pre>
<pre><code class="language-julia">result = graddescent(f, x0)
gif(result[5], &quot;graddescent.gif&quot;, fps=10);
</code></pre>
<p><img alt="gradient descent" src="../graddescent.gif" /></p>
<p>Although an appealing and intuitive idea, the above example
illustrates that gradient descent can perform surprisingly poorly in
some cases. Nonetheless, gradient descent is useful for some
problems. Notably, (stochastic) gradient descent is used to fit neural
networks, where the dimension of <code>x</code> is so large that computing the
inverse hessian in (quasi) Newton&rsquo;s method is prohibitively time
consuming. </p>
<h1 id="newtons-method">Newton&rsquo;s method<a class="headerlink" href="#newtons-method" title="Permanent link">&para;</a></h1>
<p>Newton&rsquo;s method and its variations are often the most efficient
minimization algorithms. Newton&rsquo;s method updates $x$ by minimizing a
second order approximation to $f$. Specifically:</p>
<ol>
<li>Given $x$ set $x_{new} = x - (D^2f_x)^{-1} Df_x$</li>
<li>Repeat until $\norm{Df_{x}}$, $\norm{x-x_{new}}$, and/or
   $\abs{f(x)-f(x_{new})}$ small.</li>
</ol>
<pre><code class="language-julia">markdown &amp;&amp; printfunc(&quot;newton&quot;,&quot;smooth_optimizers.jl&quot;)
</code></pre>
<pre><code>&quot;&quot;&quot;
    newton(f, x0; 
           grad=x-&gt;ForwardDiff.gradient(f,x),
           hess=x-&gt;ForwardDiff.hessian(f,x),
           ftol = 1e-6,
           xtol = 1e-4, gtol=1-6, maxiter = 1000, 
           xrange=[-2., 3.],
           yrange=[-2.,6.], animate=true)

Find the minimum of function `f` by Newton's method.

# Arguments

- `f` function to minimizie
- `x0` starting value
- `grad` function that returns gradient of `f`
- `hess` function that returns hessian of `f`
- `ftol` tolerance for function value
- `xtol` tolerance for x
- `gtol` tolerance for gradient. Convergence requires meeting all three tol
erances.
- `maxiter` maximum iterations
- `xrange` x-axis range for animation
- `yrange` y-axis range for animation
- `animate` whether to create animation

# Returns

- `(fmin, xmin, iter, info, anim)` tuple consisting of minimal function
  value, minimizer, number of iterations, convergence info, and animation

&quot;&quot;&quot;
function newton(f, x0;
                grad=x-&gt;ForwardDiff.gradient(f,x),
                hess=x-&gt;ForwardDiff.hessian(f,x),
                ftol = 1e-6,
                xtol = 1e-4, gtol=1-6, maxiter = 1000, 
                xrange=[-2., 3.],
                yrange=[-2.,6.], animate=true)
  fold = f(x0)
  xold = x0
  xchange=Inf
  fchange=Inf
  iter = 0
  stuck=0

  animate=animate &amp;&amp; length(x0)==2

  if animate
    # make a contour plot of the function we're minimizing. This is for
    # illustrating; you wouldn't have this normally
    c = contour(range(xrange[1],xrange[2], length=100),
                range(yrange[1],yrange[2], length=100),
                (x,y) -&gt; log(f([x,y])))
    anim = Animation()
  end

  g = grad(xold)

  while(iter &lt; maxiter &amp;&amp; ((xchange&gt;xtol) || (fchange&gt;ftol) || (stuck&gt;0)
                           || norm(g)&gt;gtol) )
    g = grad(xold)
    H = hess(xold)
    Δx = - inv(H)*g
    x = xold + Δx
    fnew = f(x)
    step = 1.0
    while (fnew&gt;=fold &amp;&amp; step&gt;xtol)
      step /= 1.618
      x = xold + Δx*step
      fnew = f(x)
    end

    if animate
      scatter!(c, [xold[1]],[xold[2]], markercolor=:red, legend=false,
               xlims=xrange, ylims=yrange) 
      quiver!(c, [xold[1]],[xold[2]], quiver=([Δx[1]],[Δx[2]]), legend=fals
e,
              xlims=xrange, ylims=yrange)
      frame(anim)
    end

    if (fnew&gt;=fold)
      stuck += 1
      if (stuck&gt;=10)
        break
      end
    else
      stuck = 0
      xold = x
      fold = fnew
    end
    xchange = norm(x-xold)
    fchange = abs(fnew-fold)
    iter += 1
  end
  if (iter &gt;= maxiter)
    info = &quot;Maximum iterations reached&quot;
  elseif (stuck&gt;0)
    info = &quot;Failed to improve for &quot; * string(stuck) * &quot; iterations.&quot;
  else
    info = &quot;Convergence.&quot;
  end
  return(fold, xold, iter, info, anim) 
end
</code></pre>
<pre><code class="language-julia">result = newton(f, x0)
gif(result[5], &quot;newton.gif&quot;, fps=5);
</code></pre>
<p><img alt="newton" src="../newton.gif" /></p>
<p>Newton&rsquo;s method tends to take relatively few iterations to converge
for well-behaved functions. It does have the disadvantage that hessian
and its inverse can be time consuming to compute, especially when the
dimension of $x$ is large. Newton&rsquo;s method can be unstable for
functions that are not well approximated by their second
expansion. This problem can be mitigated by combining Newton&rsquo;s method
with a line search or trust region. </p>
<h2 id="line-search">Line search<a class="headerlink" href="#line-search" title="Permanent link">&para;</a></h2>
<p>Line searches consist of approximately minimizing $f$ along a given
direction instead of updating $x$ with a fixed step size. For Newton&rsquo;s
method, instead of setting $x_{new} = x - (D^2f_x)^{-1} Df_x$, set 
$x_{new} \approx \argmin_{\delta} f(x - \delta (D^2f_x)^{-1} Df_x)$  where
$\delta$ is a scalar. This one dimensional problem can be solved
fairly quickly. Line search can also be combined with gradient
descent. </p>
<h2 id="trust-region">Trust region<a class="headerlink" href="#trust-region" title="Permanent link">&para;</a></h2>
<p>Instead of setting 
<script type="math/tex; mode=display">
x_{new} = x - (D^2f_x)^{-1} Df_x =
\argmin_{\tilde{x}} f(x) + Df_x (\tilde{x} - x) + \frac{1}{2}
(\tilde{x}-x)^T Df_x (\tilde{x} - x)
</script>
to the unconstrained minimizer of a local second order approximation,
trust region methods introduce an region near $x$ where the
approximation is trusted, and set
<script type="math/tex; mode=display">
x_{new} = \argmin_{\tilde{x} \in TR(x)} f(x) + Df_x (\tilde{x} - x) + \frac{1}{2}
(\tilde{x}-x)^T D^2 f_x (\tilde{x} - x).
</script>
Often $TR(x) = {\tilde{x} : \norm{x - \tilde{x}} &lt; r}$. The radius
of the trust region is then increased or decreased depending on
$f(x_{new})$. </p>
<h2 id="quasi-newton">Quasi-Newton<a class="headerlink" href="#quasi-newton" title="Permanent link">&para;</a></h2>
<p>Quasi-Newton methods (in particular the BFGS algorithm) are probably
the most commonly used nonlinear optimization algorithm. Quasi-Newton
methods are similar to Newton&rsquo;s method, except instead of evaluating
the hessian directly, quasi-Newton methods build an approximation to
the hessian from repeated evaluations of $Df_x$ at different $x$.</p>
<p>Optim.jl contains all the algorithms mentioned above. <a href="https://julianlsolvers.github.io/Optim.jl/stable/#user/algochoice/">Their advice on
choice of algorithm is worth
following.</a>. </p>
<h2 id="details-matter-in-practice">Details matter in practice<a class="headerlink" href="#details-matter-in-practice" title="Permanent link">&para;</a></h2>
<p>In each of the algorithms above, we were somewhat cavalier with
details like how to adjust step sizes and trust regions and what it
means to approximately minimize during a line search. In practice
these details can be quite important for how long an algorithm takes
and whether it succeeds or fails. Different implementations of
algorithms have different details. Often the details can be adjusted
through some options. It can be worthwhile to try multiple
implementations and options to get the best performance. </p>
<h1 id="constrained-optimization">Constrained optimization<a class="headerlink" href="#constrained-optimization" title="Permanent link">&para;</a></h1>
<p>Constrained optimization is a bit harder than unconstrained, but uses
similar ideas. For simple bound constraints, like $x\geq 0$ it is
often easiest to simply transform to an unconstrained case by
optimizing over $y = \log(x)$ instead. </p>
<p>For problems with equality constraints, one can apply Newton&rsquo;s method
to the first order conditions. </p>
<p>The difficult case is when there are inequality constraints. Just like
when solving analytically, the difficulty is figuring out which
constraints bind and which do not. 
For inequality constraints, we will consider problems written in the form:
<script type="math/tex; mode=display">
\min_{x \in \R^n} f(x) \text{ s.t. } c(x) \geq 0 
</script>
</p>
<h2 id="interior-point-methods">Interior Point Methods<a class="headerlink" href="#interior-point-methods" title="Permanent link">&para;</a></h2>
<p>Interior point methods circumvent the problem of figuring out which
constraints bind by approaching the optimum from the interior of the
feasible set. To do this, the interior point method applies Newton&rsquo;s
method to a modified version of the first order condition. The
unmodified first order conditions can be written
<script type="math/tex; mode=display">
\begin{align*}
0 = & Df_x - \lambda^T Dc_x \\
0 = & \lambda_i c_i(x) \\
\lambda \geq & 0 \\
c(x) \geq & 0
\end{align*}
</script>
A difficulty with these conditions is that solving them can require
guessing and checking which combinations of constraints bind and which
do not. Interior point methods get around this problem by beginning
with an interior $x$ and $\lambda$ such that $\lambda&gt;0$ and
$c(x)&gt;0$. They are then updated by applying Newton&rsquo;s method to the
equations
<script type="math/tex; mode=display">
\begin{align*}
0 = & Df_x - \lambda^T Dc_x \\
\mu = & \lambda_i c_i(x) \\
\end{align*}
</script>
where there is now a $\mu$ in place of $0$ in the second equation. $x$
and $\lambda$ are updated according to Newton&rsquo;s method for this system
of equations. In particular, 
$x_{new} = x + \Delta_x$ and $\lambda_{new}= \lambda + \Delta_\lambda$, where
<script type="math/tex; mode=display">
\begin{align*}
\begin{pmatrix} - ( Df_x - \lambda^T Dc_x) \\
\mu 1_m -  diag(c(x)) \lambda 
\end{pmatrix} = \begin{pmatrix} 
 D^2 f_x -  D^2 (\lambda c)_x  & -Dc_x^T \\
 \lambda Dc_x & diag(c(x)) 
\end{pmatrix} \begin{pmatrix}
\Delta_x \\
\Delta_\lambda
\end{pmatrix}
\end{align*}
</script>
Over iterations $\mu$ is gradually decreased toward
$0$. Here is one simple implementation.</p>
<pre><code class="language-julia">markdown &amp;&amp; printfunc(&quot;interiorpoint&quot;,&quot;constrained_optimizers.jl&quot;)
</code></pre>
<pre><code>&quot;&quot;&quot;
    interiorpoint(f, x0, c; 
                  L   = (x,λ)-&gt;(f(x) - dot(λ,c(x))),
                  ∇ₓL = (x,λ)-&gt;ForwardDiff.gradient(z-&gt;L(z,λ), x),
                  ∇²ₓL= (x,λ)-&gt;ForwardDiff.hessian(z-&gt;L(z,λ), x),
                  ∇c = x-&gt;ForwardDiff.jacobian(c,x),
                  tol=1e-4, maxiter = 1000,
                  μ0 = 1.0, μfactor = 0.2,
                  xrange=[-2., 3.],
                  yrange=[-2.,6.], animate=true)

Find the minimum of function `f` subject to `c(x) &gt;= 0` using a
primal-dual interior point method.

# Arguments

- `f` function to minimizie
- `x0` starting value. Must have c(x0) &gt; 0
- `c` constraint function. Must return an array.
- `L   = (x,λ)-&gt;(f(x) - dot(λ,c(x)))` Lagrangian
- `∇ₓL = (x,λ)-&gt;ForwardDiff.gradient(z-&gt;L(z,λ), x)` Derivative of Lagrangia
n wrt `x`
- `∇²ₓL= (x,λ)-&gt;ForwardDiff.hessian(z-&gt;L(z,λ), x)` Hessian of Lagrangian wr
t `x`
- `∇c = x-&gt;ForwardDiff.jacobian(c,x)` Jacobian of constraints
- `tol` convergence tolerance
- `μ0` initial μ
- `μfactor` how much to decrease μ by
- `xrange` range of x-axis for animation
- `yrange` range of y-axis for animation
- `animate` whether to create an animation (if true requires length(x)==2)
- `verbosity` higher values result in more printed output during search. 0 
for no output, any number &gt; 0 for some.  

# Returns

- `(fmin, xmin, iter, info, animate)` tuple consisting of minimal function
  value, minimizer, number of iterations, and convergence info

&quot;&quot;&quot;
function interiorpoint(f, x0, c;
                       L   = (x,λ)-&gt;(f(x) - dot(λ,c(x))),
                       ∇ₓL = (x,λ)-&gt;ForwardDiff.gradient(z-&gt;L(z,λ), x),
                       ∇²ₓL= (x,λ)-&gt;ForwardDiff.hessian(z-&gt;L(z,λ), x),
                       ∇c = x-&gt;ForwardDiff.jacobian(c,x),
                       tol=1e-4, maxiter = 1000,
                       μ0 = 1.0, μfactor = 0.2,
                       xrange=[-2., 3.],
                       yrange=[-2.,6.], animate=true, verbosity=0)
  fold = f(x0)
  xold = x0
  all(c(x0).&gt;0) || error(&quot;interiorpoint requires a starting value that stri
ctly satisfies all constraints&quot;)
  μ = μ0
  λ = μ./c(x0)
  xchange=Inf
  fchange=Inf
  iter = 0
  μiter = 0
  stuck=0

  animate = animate &amp;&amp; length(x0)==2
  if animate
    # make a contour plot of the function we're minimizing. This is for
    # illustrating; you wouldn't have this normally
    ct = contour(range(xrange[1],xrange[2], length=100), 
                range(yrange[1],yrange[2], length=100),
                 (x,y) -&gt; log(f([x,y])))
    plot!(ct, xrange, 2.5 .- xrange) # add constraint 
    anim = Animation()
  end
  foc = [∇ₓL(xold,λ); λ.*c(xold)]
  while(iter &lt; maxiter &amp;&amp; ((xchange&gt;tol) || (fchange&gt;tol) || (stuck&gt;0)
                           || norm(foc)&gt;tol || μ&gt;tol) )
    # Calculate the direction for updating x and λ
    Dc = ∇c(xold)
    cx = c(xold)
    foc = ∇ₓL(xold, λ)
    H = ∇²ₓL(xold,λ)
    Δ = [H   -Dc'; λ'*Dc  diagm(cx)] \ [-foc; μ .- cx.*λ]

    # Find a step size such that λ&gt;=0 and c(x)&gt;=0
    # The details here could surely be improved
    α = 1.0
    acceptedstep = false
    λold = copy(λ)
    x = copy(xold)
    while (α &gt; 1e-10)
      x = xold + α*Δ[1:length(xold)]
      λ = λold + α*Δ[(length(xold)+1):length(Δ)]
      if (all(λ.&gt;=0) &amp;&amp; all(c(x).&gt;=0))
        acceptedstep=true
        break
      end
      α *= 0.5
    end
    if !acceptedstep
      stuck = 1
      break
    end
    fnew = f(x)

    if (animate)
      scatter!(ct, [xold[1]],[xold[2]], markercolor=:red, legend=false,
               xlims=xrange, ylims=yrange) 
      quiver!(ct, [xold[1]],[xold[2]], quiver=([α*Δ[1]],[α*Δ[2]]), legend=f
alse,
              xlims=xrange, ylims=yrange)
      frame(anim)
    end

    xchange = norm(x-xold)
    fchange = abs(fnew-fold)
    μiter += 1

    # update μ (the details here could also be improved)    
    foc = ∇ₓL(x,λ)
    if (μiter&gt;10 || (norm(foc)&lt; μ &amp;&amp; λ'*c(x)&lt;10*μ)) 
      μ *=  μfactor
      μiter = 0
    end

    xold = x
    fold = fnew
    if verbosity&gt;0
      print(&quot;Iter $iter: f=$fnew, λ=$λ, c(x)=$(c(x)), μ=$μ, norm(foc)=$(nor
m(foc))\n&quot;)
    end
    iter += 1    
  end
  if (iter &gt;= maxiter)
    info = &quot;Maximum iterations reached&quot;
  elseif (stuck&gt;0)
    info = &quot;Failed to find feasible step for &quot; * string(stuck) * &quot; iteratio
ns.&quot;
  else
    info = &quot;Convergence.&quot;
  end
  return(fold, xold, iter, info, anim) 
end
</code></pre>
<pre><code class="language-julia">f = banana(1.0,1.0)
x0 = [3.0, 0.0]
function constraint(x)
  [x[1] + x[2] - 2.5]
end
</code></pre>
<pre><code>constraint (generic function with 1 method)
</code></pre>
<pre><code class="language-julia">result = interiorpoint(f, x0, constraint; maxiter=100)
gif(result[5], &quot;ip.gif&quot;, fps=5);
</code></pre>
<p><img alt="interior point" src="../ip.gif" /></p>
<p>Optim.jl includes an interior point method. IPOPT is another popular
implementation. As above, the details of the algorithm can be
important in practice. It can be worthwhile to experiment with
different methods for updating $\mu$, using a more sophisticated line
search or trust region, and perhaps replacing the computation of the
hessian with a quasi-Newton approximation. </p>
<p>It has been proven that interior point methods converge relatively
quickly for convex optimization problems. </p>
<h2 id="sequential-quadratic-programming">Sequential quadratic programming<a class="headerlink" href="#sequential-quadratic-programming" title="Permanent link">&para;</a></h2>
<p>Sequential quadratic programming relies on the fact that there are
efficient methods to compute the solution to quadratic programs &mdash;
optimization problems with quadratic objective functions and linear
constraints. We can then solve a more general optimization problem by
solving a sequence of quadratic programs that approximate the original problem.</p>
<p>Sequential quadratic programming is like a constrained version of
Newton&rsquo;s method. Given a current $x$ and $\lambda$ the new $x$ solves
<script type="math/tex; mode=display">
\begin{align*}
x_{new} \in \argmin_{\tilde{x}} & f(x) + Df_x (\tilde{x} - x) +
\frac{1}{2} (\tilde{x}-x)^T (D^2 f_x + D^2 (\lambda^T c)_x) (\tilde{x} - x) \\
 \text{ s. t. } & c(x) + Dc_{x} (\tilde{x} - x) \geq 0
\end{align*}
</script>
and the new $\lambda$ is set to the value of the multipliers for this
problem. </p>
<p>This quadratic program (an optimization problem with a quadratic
objective function and linearc onstraints) can be solved fairly
efficiently if $(D^2 f_x + D^2 (\lambda^T c)_x)$ is positive
semi-definite. </p>
<div class="admonition info convex program solvers">
<p class="admonition-title">Info</p>
<p>Most for Convex program solvers are designed to accept semidefinite
programs instead of quadratic programs. A <a href="https://math.stackexchange.com/q/2256243">quadratic program can be
re-written as a semidefinite
program</a>. A solver such as
SCS, ECOS, or Mosek can then be used. Fortunately, <code>Convex.jl</code> will
automatically take care of any necessary transformation.</p>
</div>
<p>One could also incorporate a trust region or line search into the
above algorithm. Here is one simple implementation.</p>
<pre><code class="language-julia">using Convex, ECOS
</code></pre>
<pre><code>Error: ArgumentError: Package Convex not found in current path:
- Run `import Pkg; Pkg.add(&quot;Convex&quot;)` to install the Convex package.
</code></pre>
<pre><code class="language-julia">markdown &amp;&amp; printfunc(&quot;sequentialquadratic&quot;,&quot;constrained_optimizers.jl&quot;)
</code></pre>
<pre><code>&quot;&quot;&quot;
      sequentialquadratic(f, x0, c; 
                          ∇f = x-&gt;ForwardDiff.gradient(f,x),
                          ∇c = x-&gt;ForwardDiff.jacobian(c,x),
                          L   = (x,λ)-&gt;(f(x) - dot(λ,c(x))),
                          ∇ₓL = (x,λ)-&gt;ForwardDiff.gradient(z-&gt;L(z,λ), x),
                          ∇²ₓL= (x,λ)-&gt;ForwardDiff.hessian(z-&gt;L(z,λ), x),
                          tol=1e-4, maxiter = 1000,
                          trustradius=1.0, xrange=[-2., 3.],
                          yrange=[-2.,6.], animate=true, verbosity=1)


Find the minimum of function `f` subject to `c(x) ≥ 0` by sequential quadra
tic programming.

# Arguments

- `f` function to minimizie
- `x0` starting value. Must have c(x0) &gt; 0
- `c` constraint function. Must return an array.
- `∇f = x-&gt;ForwardDiff.gradient(f,x)`
- `∇c = x-&gt;ForwardDiff.jacobian(c,x)` Jacobian of constraints
- `L   = (x,λ)-&gt;(f(x) - dot(λ,c(x)))` Lagrangian
- `∇ₓL = (x,λ)-&gt;ForwardDiff.gradient(z-&gt;L(z,λ), x)` Derivative of Lagrangia
n wrt `x`
- `∇²ₓL= (x,λ)-&gt;ForwardDiff.hessian(z-&gt;L(z,λ), x)` Hessian of Lagrangian wr
t `x`
- `tol` convergence tolerance
- `maxiter`
- `trustradius` initial trust region radius
- `xrange` range of x-axis for animation
- `yrange` range of y-axis for animation
- `animate` whether to create an animation (if true requires length(x)==2)
- `verbosity` higher values result in more printed output during search. 0 
for no output, any number &gt; 0 for some.  

# Returns

- `(fmin, xmin, iter, info, animate)` tuple consisting of minimal function
  value, minimizer, number of iterations, and convergence info

&quot;&quot;&quot;
function sequentialquadratic(f, x0, c;
                             ∇f = x-&gt;ForwardDiff.gradient(f,x),
                             ∇c = x-&gt;ForwardDiff.jacobian(c,x),
                             L   = (x,λ)-&gt;(f(x) - dot(λ,c(x))),
                             ∇ₓL = (x,λ)-&gt;ForwardDiff.gradient(z-&gt;L(z,λ), x
),
                             ∇²ₓL= (x,λ)-&gt;ForwardDiff.hessian(z-&gt;L(z,λ), x)
,
                             tol=1e-4, maxiter = 1000,
                             trustradius=1.0,
                             xrange=[-2., 3.],
                             yrange=[-2.,6.], animate=true, verbosity=1)
  fold = f(x0)
  xold = x0
  xchange=Inf
  fchange=Inf
  iter = 0
  μiter = 0
  stuck=0

  animate = animate &amp;&amp; length(x0)==2
  if animate
    # make a contour plot of the function we're minimizing. This is for
    # illustrating; you wouldn't have this normally
    ct = contour(range(xrange[1],xrange[2], length=100), 
                range(yrange[1],yrange[2], length=100),
                 (x,y) -&gt; log(f([x,y])))
    plot!(ct, xrange, 2.5 .- xrange) # add constraint 
    anim = Animation()
  end
  Dc = ∇c(xold)
  Df = ∇f(xold)
  λ = (Dc*Dc') \ Dc*Df
  foc = ∇ₓL(xold,λ)
  fold  = f(xold)
  negsquared(x) = x &lt; 0 ? x^2 : zero(x)
  merit(x) = f(x) + 1000.0*sum(negsquared.(c(x)))
  while(iter &lt; maxiter &amp;&amp; ((xchange&gt;tol) || (fchange&gt;tol) || (stuck&gt;0)
                           || norm(foc)&gt;tol) )
    Df = ∇f(xold)
    Dc = ∇c(xold)
    cx = c(xold)
    H = ∇²ₓL(xold,λ)

    # QP will fail with if H not positive definite
    if !(isposdef(H))
      v,Q = eigen(H)
      Hp = Symmetric(Q*Diagonal(max.(v,one(eltype(v))*1e-8))*Q')
    else
      Hp = H
    end
    # set up and solve our QP
    Δ = Variable(length(xold))
    problem = minimize(Df'*Δ + quadform(Δ,Hp), [cx + Dc*Δ &gt;= 0; norm(Δ)&lt;=tr
ustradius])
    solve!(problem, ECOS.Optimizer(verbose=verbosity))
    λ .= problem.constraints[1].dual
    xnew = xold .+ Δ.value

    if (animate)
      scatter!(ct, [xold[1]],[xold[2]], markercolor=:red, legend=false,
               xlims=xrange, ylims=yrange) 
      quiver!(ct, [xold[1]],[xold[2]], quiver=([Δ.value[1]],[Δ.value[2]]), 
legend=false,
              xlims=xrange, ylims=yrange)
      frame(anim)
    end


    # decide whether to accept new point and whether to adjust trust region
    if (merit(xnew) &lt; merit(xold))
      stuck = 0
      foc = [∇ₓL(xold,λ); λ.*c(xold)]
      if (problem.constraints[2].dual&gt;1e-4) # trust region binding
        trustradius *= 3/2
      end
    else
      stuck += 1
      trustradius *= 2/3
      if (stuck&gt;=20)
        break
      end
    end


    xchange = norm(xnew-xold)
    fchange = abs(f(xnew)-f(xold))
    xold = xnew

    if verbosity&gt;0
      print(&quot;Iter $iter: f=$(f(xold)), λ=$λ, c(x)=$(c(xold)), TR=$trustradi
us, norm(foc)=$(norm(foc))\n&quot;)
    end
    iter += 1    
  end
  if (iter &gt;= maxiter)
    info = &quot;Maximum iterations reached&quot;
  elseif (stuck&gt;0)
    info = &quot;Failed to find feasible step for &quot; * string(stuck) * &quot; iteratio
ns.&quot;
  else
    info = &quot;Convergence.&quot;
  end
  return(f(xold), xold, iter, info, anim) 
end
</code></pre>
<pre><code class="language-julia">x0 = [0.0, 6.0]
result = sequentialquadratic(f, x0, constraint; maxiter=100, verbosity=0);
gif(result[5], &quot;sqp.gif&quot;, fps=5);
</code></pre>
<p><img alt="sqp" src="../sqp.gif" /></p>
<p>Compared to interior point methods, sequential quadratic programming
has the advantage of not needing a feasible point to begin, and often
taking fewer iteration. Like Newton&rsquo;s method, sequential quadratic
programming has local quadratic convergence. A downside of sequential
quadratic programming is that solving the quadratic program at each
step can take considerably longer than solving the system of linear
equations that interior point methods and Newton methods require.</p>
<h2 id="slqp-active-set">SLQP active Set<a class="headerlink" href="#slqp-active-set" title="Permanent link">&para;</a></h2>
<p>SLQP active set methods use a linear approximation to the optimization
problem to decide which constraints are &ldquo;active&rdquo; (binding). In each
iteration, a linear approximation to the original problem is first
solved. The constraints that bind in linear approximation are then
assumed to bind in the full problem, and we solve an equality
constrained quadratic program to determine the next step.</p>
<p>Byrd and Waltz (2011)[bryd2011] for more details and an extension to
the SLQP method. </p>
<pre><code class="language-julia">markdown &amp;&amp; printfunc(&quot;slqp&quot;,&quot;constrained_optimizers.jl&quot;)
</code></pre>
<pre><code>&quot;&quot;&quot;
      slqp(f, x0, c; 
           ∇f = x-&gt;ForwardDiff.gradient(f,x),
           ∇c = x-&gt;ForwardDiff.jacobian(c,x),
           L   = (x,λ)-&gt;(f(x) - dot(λ,c(x))),
           ∇ₓL = (x,λ)-&gt;ForwardDiff.gradient(z-&gt;L(z,λ), x),
           ∇²ₓL= (x,λ)-&gt;ForwardDiff.hessian(z-&gt;L(z,λ), x),
           tol=1e-4, maxiter = 1000,
           trustradius=1.0, xrange=[-2., 3.],
           yrange=[-2.,6.], animate=true, verbosity=1)


Find the minimum of function `f` subject to `c(x) ≥ 0` by sequential
linear quadratic programming. 

See
https://en.wikipedia.org/wiki/Sequential_linear-quadratic_programming
for algorithm information.

# Arguments

- `f` function to minimizie
- `x0` starting value. Must have c(x0) &gt; 0
- `c` constraint function. Must return an array.
- `∇f = x-&gt;ForwardDiff.gradient(f,x)`
- `∇c = x-&gt;ForwardDiff.jacobian(c,x)` Jacobian of constraints
- `L   = (x,λ)-&gt;(f(x) - dot(λ,c(x)))` Lagrangian
- `∇ₓL = (x,λ)-&gt;ForwardDiff.gradient(z-&gt;L(z,λ), x)` Derivative of Lagrangia
n wrt `x`
- `∇²ₓL= (x,λ)-&gt;ForwardDiff.hessian(z-&gt;L(z,λ), x)` Hessian of Lagrangian wr
t `x`
- `tol` convergence tolerance
- `maxiter`
- `trustradius` initial trust region radius
- `xrange` range of x-axis for animation
- `yrange` range of y-axis for animation
- `animate` whether to create an animation (if true requires length(x)==2)
- `verbosity` higher values result in more printed output during search. 0 
for no output, any number &gt; 0 for some.  

# Returns

- `(fmin, xmin, iter, info, animate)` tuple consisting of minimal function
  value, minimizer, number of iterations, and convergence info

&quot;&quot;&quot;
function slqp(f, x0, c;
              ∇f = x-&gt;ForwardDiff.gradient(f,x),
              ∇c = x-&gt;ForwardDiff.jacobian(c,x),
              L   = (x,λ)-&gt;(f(x) - dot(λ,c(x))),
              ∇ₓL = (x,λ)-&gt;ForwardDiff.gradient(z-&gt;L(z,λ), x),
              ∇²ₓL= (x,λ)-&gt;ForwardDiff.hessian(z-&gt;L(z,λ), x),
              tol=1e-4, maxiter = 1000,
              trustradius=1.0,
              xrange=[-2., 3.],
              yrange=[-2.,6.], animate=true, verbosity=1)
  fold = f(x0)
  xold = x0
  xchange=Inf
  fchange=Inf
  iter = 0
  μiter = 0
  stuck=0
  lptrustradius = trustradius

  animate = animate &amp;&amp; length(x0)==2
  if animate
    # make a contour plot of the function we're minimizing. This is for
    # illustrating; you wouldn't have this normally
    ct = contour(range(xrange[1],xrange[2], length=100), 
                range(yrange[1],yrange[2], length=100),
                 (x,y) -&gt; log(f([x,y])))
    plot!(ct, xrange, 2.5 .- xrange) # add constraint 
    anim = Animation()
  end
  Dc = ∇c(xold)
  Df = ∇f(xold)
  cx = c(xold)
  λ = (Dc*Dc') \ Dc*Df
  foc = [∇ₓL(xold,λ); λ.*cx]
  fold  = f(xold)
  negsquared(x) = x &lt; 0 ? x^2 : zero(x)
  merit(x) = f(x) + sum(negsquared.(c(x)))
  while(iter &lt; maxiter &amp;&amp; ((xchange&gt;tol) || (fchange&gt;tol) || (stuck&gt;0)
                           || (norm(foc)&gt;tol)) )
    Df = ∇f(xold)
    Dc = ∇c(xold)
    cx = c(xold)
    H = ∇²ₓL(xold,λ)

    # set up and solve linear program for finding active set (binding
    # constraints)
    Δ = Variable(length(xold))
    problem = minimize(Df'*Δ, [cx + Dc*Δ &gt;= 0,
                               Δ &lt;= lptrustradius,
                               -lptrustradius &lt;= Δ])
    solve!(problem, ECOS.Optimizer(verbose=verbosity))
    while (Int(problem.status)!=1 &amp;&amp; lptrustradius &lt;=1e4)
      lptrustradius *= 2
      problem = minimize(Df'*Δ, [cx + Dc*Δ &gt;= 0,
                               Δ &lt;= lptrustradius,
                               -lptrustradius &lt;= Δ])
      solve!(problem, ECOS.Optimizer(verbose=verbosity))
    end

    if isa(problem.constraints[1].dual, AbstractArray)
      active = vec(problem.constraints[1].dual .&gt; tol)
    else
      active = [problem.constraints[1].dual .&gt; tol]
    end

    # set up and solve our QP
    Δ = Variable(length(xold))
    if !(isposdef(H))
      Hp = zero(H)
    else
      Hp = H
    end
    if any(active) 
      problem = minimize(Df'*Δ + 0.5*quadform(Δ,Hp), [cx[active] + Dc[activ
e,:]*Δ &gt;= 0, norm(Δ)&lt;=trustradius])
      solve!(problem, ECOS.Optimizer(verbose=verbosity))
      λ[active] .= problem.constraints[1].dual
      λ[.!active] .= zero(eltype(λ))
    else
      problem = minimize(Df'*Δ + 0.5*quadform(Δ,Hp), [norm(Δ)&lt;=trustradius]
)
      solve!(problem, ECOS.Optimizer(verbose=verbosity))
      λ = zero(cx)
    end
    xnew = xold .+ Δ.value

    if (animate)
      scatter!(ct, [xold[1]],[xold[2]], markercolor=:red, legend=false,
               xlims=xrange, ylims=yrange) 
      quiver!(ct, [xold[1]],[xold[2]], quiver=([Δ.value[1]],[Δ.value[2]]), 
legend=false,
              xlims=xrange, ylims=yrange)
      frame(anim)
    end


    # decide whether to accept new point and whether to adjust trust region
    if (merit(xnew)  &lt; merit(xold))
      stuck = 0
      foc = [∇ₓL(xold,λ); λ.*c(xold)]
      if (problem.constraints[end].dual&gt;1e-4) # trust region binding
        trustradius *= 3/2
      end
    else
      stuck += 1
      trustradius *= 2/3
      if (stuck&gt;=20)
        break
      end
    end

    xchange = norm(xnew-xold)
    fchange = abs(f(xnew)-f(xold))

    xold = xnew

    if verbosity&gt;0
      print(&quot;Iter $iter: f=$(f(xold)), λ=$λ, c(x)=$(c(xold)),TR=$trustradiu
s, norm(foc)=$(norm(foc)), $xchange , $fchange, $stuck\n&quot;)
    end
    iter += 1    
  end
  if (iter &gt;= maxiter)
    info = &quot;Maximum iterations reached&quot;
  elseif (stuck&gt;0)
    info = &quot;Failed to find feasible step for &quot; * string(stuck) * &quot; iteratio
ns.&quot;
  else
    info = &quot;Convergence.&quot;
  end
  return(f(xold), xold, iter, info, anim) 
end
</code></pre>
<pre><code class="language-julia">x0 = [-1.0, 6.0]
result = slqp(f, x0, constraint; maxiter=100, verbosity=0);
gif(result[5], &quot;slqp.gif&quot;, fps=5);
</code></pre>
<p><img alt="slqp" src="../slqp.gif" /></p>
<h2 id="augmented-lagrangian">Augmented Lagrangian<a class="headerlink" href="#augmented-lagrangian" title="Permanent link">&para;</a></h2>
<p>Augmented Lagragian methods convert a constrained minimization problem
to an unconstrained problem by adding a penalty that increases with
the constraint violation to the Lagrangian. </p>
<h2 id="barrier-methods">Barrier methods<a class="headerlink" href="#barrier-methods" title="Permanent link">&para;</a></h2>
<p>Barrier methods refer to adding a penalty that increases toward
$\infty$ as the constraints get close to violated (such as
$\log(c(x))$). Barrier methods are closely related to interior point
methods. Applying Newton&rsquo;s method to a log-barrier penalized problem
gives rise to something very similar to our <code>interiorpoint</code> algorithm
above. </p>
<h1 id="strategies-for-global-optimization">Strategies for global optimization<a class="headerlink" href="#strategies-for-global-optimization" title="Permanent link">&para;</a></h1>
<p>The above algorithms will all converge to local minima. Finding a
global minimum is generally very hard. There are a few algorithms that
have been proven to converge to a global optimum, such a DIRECT-L in
<code>NLopt</code>. However, these algorithms are prohibitively time-consuming
for even moderate size problems. </p>
<p>Randomization is a good strategy for avoiding local minima. Some
algorithms with randomization, like simulated annealing, can be shown
to converge to the global optimum with high probability. In practice,
these are also often too inefficient for moderate size
problems. </p>
<p>A good practical approach is to use an algorithm that combines
randomization with some model-based search. A common approach is to
use a variant of Newton&rsquo;s method starting from a bunch of
randomly chosen initial values. </p>
<p>Algorithms that combine a linear or quadratic approximation to the
objective function with some randomness in the search direction can
also be useful. An example is stochastic gradient descent, which is
often used to fit neural networks. I have had good experience with
<a href="http://cma.gforge.inria.fr/">CMA-ES</a>. It worked well to estimate the
finite mixture model in EFS (2015)<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>. </p>
<p>Bayesian methods can also be used for optimization and will naturally
include some randomization in their search. Hamiltonian Monte-Carlo
methods, which incorporate gradient information in their search, are
likely to be efficient. See
<a href="https://github.com/tpapp/DynamicHMC.jl"><code>DynamicHMC.jl</code></a>.</p>
<h1 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h1>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Liran Einav, Amy Finkelstein, and Paul Schrimpf.  The Response of Drug Expenditure to Nonlinear Contract Design: Evidence from Medicare Part D *. <em>The Quarterly Journal of Economics</em>, 130<script type="math/tex">2</script>:841&ndash;899, 02 2015. URL: <a href="https://doi.org/10.1093/qje/qjv005">https://doi.org/10.1093/qje/qjv005</a>, <a href="https://arxiv.org/abs/http://oup.prod.sis.lan/qje/article-pdf/130/2/841/17097031/qjv005.pdf">arXiv:http://oup.prod.sis.lan/qje/article-pdf/130/2/841/17097031/qjv005.pdf</a>, <a href="https://doi.org/10.1093/qje/qjv005">doi:10.1093/qje/qjv005</a>.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../assets/mathjaxhelper.js" defer></script>

        <div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
